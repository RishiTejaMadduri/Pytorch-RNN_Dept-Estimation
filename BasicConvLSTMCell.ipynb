{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "Activation=nn.Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNNCell(nn.Module):\n",
    "    #Abstract Object Representing a Convolutional RNN Cell\n",
    "    \n",
    "    def __call__(self, input, stare, scope=None):\n",
    "        #Run this RNN cell on inputs, starting from given state.\n",
    "        \n",
    "        raise NotImplementedError(\"Abstract Method\")\n",
    "    def state_size(self):\n",
    "        #size(s) of state(s) used by this cell.\n",
    "        raise NotImplementedError(\"Abstract Method\")\n",
    "        \n",
    "    def output_size(self):\n",
    "        #Integer or TensorShape: size of outputs produced by this cell.\"\"\"\n",
    "        raise NotImplementedError(\"Abstract method\")\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        \"\"\"Return zero-filled state tensor(s).\n",
    "        Args:\n",
    "          batch_size: int, float, or unit Tensor representing the batch size.\n",
    "          dtype: the data type to use for the state.\n",
    "        Returns:\n",
    "          tensor of shape '[batch_size x shape[0] x shape[1] x num_features]\n",
    "          filled with zeros\n",
    "        \"\"\"\n",
    "        shape = self.shape \n",
    "        num_features = self.num_features\n",
    "        zeros = torch.zeros([batch_size, shape[0], shape[1], num_features * 2]) \n",
    "        return zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConvLSTMCell(ConvRNNCell):\n",
    "    #Basic Conv LSTM recurrent network cell\n",
    "    \n",
    "    def __init__(self, shape, filter_size, num_features, forget_bias=1.0, input_size=None,\n",
    "               state_is_tuple=False, activation=Activation):\n",
    "        \"\"\"Initialize the basic Conv LSTM cell.\n",
    "        Args:\n",
    "          shape: int tuple thats the height and width of the cell\n",
    "          \n",
    "          filter_size: int tuple-height & width of the filter\n",
    "          \n",
    "          num_features: int-depth of the cell \n",
    "          forget_bias: floa-The bias added to forget gates (see above).\n",
    "          input_size: Deprecated and unused.\n",
    "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "            the `c_state` and `m_state`.  If False, they are concatenated\n",
    "            along the column axis.  The latter behavior will soon be deprecated.\n",
    "          activation: Activation function of the inner states.\n",
    "        \"\"\"\n",
    "        \n",
    "        #if not state_is_tuple:\n",
    "        #logging.warn(%s: Using a concatenated state is slower and will soon be\"\n",
    "        #              \"deprecated. Use state_is_tuple=True.\", self)\n",
    "        \n",
    "        if input_size is not None:\n",
    "            logging.warn(\"%s: The input_size parameter is deprecated.\", self)\n",
    "            \n",
    "        self.shape=shape\n",
    "        self.filter_size=filter_size\n",
    "        self.num_features=num_features\n",
    "        self._forget_bias=forget_bias\n",
    "        self._state_is_tuple=state_is_tuple\n",
    "        self._activation=activation\n",
    "        \n",
    "        def state_size(self):\n",
    "            return (nn.LSTM(slef._num_units, self._num_units)\n",
    "                    if self._state_is_tuple else 2*self._num_units)\n",
    "        \n",
    "        def output_size(self):\n",
    "            return self._num_units\n",
    "        \n",
    "        def __call__(self, inputs, state, scope=None):\n",
    "            \"\"\"LSTM cell\"\"\"\n",
    "            super(BasicConvLSTMCell, self).__init__()\n",
    "            \n",
    "            if self.__state_is_tuple:\n",
    "                c, h = state\n",
    "            else:\n",
    "                c, h = torch.split(state, split_size_or_sections=2, dim=3)\n",
    "            \n",
    "            concat = _conv_linear([inputs,h], self.filter_size, self.num_features*4, True) \n",
    "            \n",
    "            i, j, f, o = torch.split(concat, split_size_or_sections=4, dim=3)\n",
    "            \n",
    "            new_c = (c * F.sigmoid(f+self_.forget_bias)+F.sigmoid(i)*self._activation(j))\n",
    "            \n",
    "            new_h = self._activation(new_c)*F.sigmoid(o)\n",
    "            \n",
    "            if self._state_is_tuple:\n",
    "                new_state = nn.LSTM(new_c, new_h)\n",
    "                    \n",
    "            else:\n",
    "                new_state = torch.cat((new_c, new_h), 3)\n",
    "                \n",
    "            return new_h, new_state\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conv_linear(args, filter_size, num_features, bias, bias_start=0.0, scope=None):\n",
    "    \"\"\"convolution:\n",
    "    Args:\n",
    "    args: a 4D Tensor or a list of 4D, batch x n, Tensors.\n",
    "    filter_size: int tuple of filter height and width.\n",
    "    num_features: int, number of features.\n",
    "    bias_start: starting value to initialize the bias; 0 by default.\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "    Returns:\n",
    "    A 4D Tensor with shape [batch h w num_features]\n",
    "    Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  \"\"\"\n",
    "    #Calculate the total size of arguments on dimension 1\n",
    "    total_args_size_depth=0\n",
    "    shapes=[a.get_shape().as_lists() for a in args]\n",
    "    \n",
    "    for shape in shapes:\n",
    "        if len(shapes)!=4:\n",
    "            raise ValueError(\"Linear is expecting 4D arguments: %s\" %str(shapes))\n",
    "            \n",
    "        if not shape[3]:\n",
    "            raise ValueError(\"Linear expects shape[4] of arguments: %s\" %str(shapes))\n",
    "            \n",
    "        else:\n",
    "            total_arg_size_depth+=shape[3]\n",
    "        \n",
    "    dtype=[a.dtype for a in args][0]\n",
    "    \n",
    "    matrix=torch.empty([filter_size[0], filter_size[1], total_arg_size_depth, num_features], dtype=dtype)\n",
    "    \n",
    "    if len(args)==1:\n",
    "        res=nn.Conv2d(args[0], matrix, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    else:\n",
    "        res=nn.Conv2d(torch.cat((args),3),matrix, strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "    if not bias:\n",
    "        return res\n",
    "    \n",
    "    bias_term=torch.empty([num_features], dtype=dtype, initializer=nn.linear(bias_start, dtype=dtype))\n",
    "    \n",
    "    return res+bias_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
