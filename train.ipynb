{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "from matplotlib import cm\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "# from rnn_depth trainer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description = \"Train RNN Depth\")\n",
    "parser.add_argument(\"--dataset_dir\", type= str, default=\"\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"\")\n",
    "parser.add_argument(\"--continue_train\", type = bool, default= \"false\")\n",
    "parser.add_argument(\"--eval_set_dir\", type=str, default=None, help=\"The path to the evaluation directory\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=20, help=\"The number of training epochs\")\n",
    "parser.add_argument(\"--summary_freq\", type=int, default=100, help=\"The frequence to summarize and save model\")\n",
    "parser.add_argument(\"--eval_freq\", type=int, default=1000, help=\"The frequence to evaluate model\")\n",
    "parser.add_argument(\"--save_latest_freq\", type=int, default=5000, help=\"The frequence to save model\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #Write save module\n",
    "    \n",
    "    tb_writer = SummaryWriter(args.save_path)\n",
    "    \n",
    "    # Initialize trainer object\n",
    "    m_trainer = rnn_depth_trainer()\n",
    "    \n",
    "    # Initialize data loading object\n",
    "    dataLoader = m_trainer.initDataloader(args.dataset_dir, num_epochs=args.num_epochs)\n",
    "    \n",
    "    if args.eval_set_dir is not None:\n",
    "        eval_dataLoader = m_trainer.initDataloader(args.eval_set_dir, num_epochs=None,is_training=False)\n",
    "        \n",
    "    \n",
    "    print(\"~~~Creating Model~~~\")\n",
    "    model = models.rnn_depth_decoderlstm().to(device)\n",
    "    \n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    print(\"~~~Setting Adam Solver~~~\")\n",
    "    \n",
    "    optim_params = [\n",
    "        {'params:' model.parameters(), 'lr': agrs.lr}\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(optim_params, beats = (args.momentum, args.beta), weight_decay = args.weight_decay)\n",
    "    \n",
    "    #Change according to your loss function\n",
    "    with open(args.save_path/args.log_summary, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        writer.writerow(['train_loss', 'validation_loss'])\n",
    "\n",
    "    with open(args.save_path/args.log_full, 'w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t')\n",
    "        writer.writerow(['train_loss', 'photo_loss', 'explainability_loss', 'smooth_loss'])\n",
    "    \n",
    "    #Check if necessary and remove if not\n",
    "    logger = TermLogger(n_epochs=args.epochs, train_size=min(len(train_loader), args.epoch_size), valid_size=len(val_loader))\n",
    "    logger.epoch_bar.start()\n",
    "    \n",
    "    logger.reset_valid_bar()\n",
    "    \n",
    "    #Change as needed \n",
    "    if args.pretrained_disp or args.evaluate:\n",
    "        \n",
    "        logger.reset_valid_bar()\n",
    "        if args.with_gt:\n",
    "            errors, error_names = validate_with_gt(args, val_loader, disp_net, 0, logger, tb_writer)\n",
    "        else:\n",
    "            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, 0, logger, tb_writer)\n",
    "        for error, name in zip(errors, error_names):\n",
    "            tb_writer.add_scalar(name, error, 0)\n",
    "        error_string = ', '.join('{} : {:.3f}'.format(name, error) for name, error in zip(error_names[2:9], errors[2:9]))\n",
    "        logger.valid_writer.write(' * Avg {}'.format(error_string))    \n",
    "        \n",
    "    for epoch in range(args.epoch):\n",
    "        logger.epoch_bar.update(epoch)\n",
    "        \n",
    "        #train for one epoch\n",
    "        logger.rest_train_bar()\n",
    "        \n",
    "        train_loss = train(args, train_loader, disp_net, pose_exp_net, optimizer, args.epoch_size, logger, tb_writer)\n",
    "        logger.train_writer.write(' * Avg Loss : {:.3f}'.format(train_loss))\n",
    "        \n",
    "         # evaluate on validation set - Check evaluation code and change if needed\n",
    "        logger.reset_valid_bar()\n",
    "        if args.with_gt:\n",
    "            errors, error_names = validate_with_gt(args, val_loader, disp_net, epoch, logger, tb_writer)\n",
    "        else:\n",
    "            errors, error_names = validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, tb_writer)\n",
    "        error_string = ', '.join('{} : {:.3f}'.format(name, error) for name, error in zip(error_names, errors))\n",
    "        logger.valid_writer.write(' * Avg {}'.format(error_string))\n",
    "\n",
    "        for error, name in zip(errors, error_names):\n",
    "            tb_writer.add_scalar(name, error, epoch)\n",
    "            \n",
    "        # Up to you to chose the most relevant error to measure your model's performance, careful some measures are to maximize (such as a1,a2,a3)\n",
    "        decisive_error = errors[1]\n",
    "        if best_error < 0:\n",
    "            best_error = decisive_error\n",
    "\n",
    "        # remember lowest error and save checkpoint- Change for your model\n",
    "        is_best = decisive_error < best_error\n",
    "        best_error = min(best_error, decisive_error)\n",
    "        save_checkpoint(\n",
    "            args.save_path, {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': disp_net.module.state_dict()\n",
    "            }, {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': pose_exp_net.module.state_dict()\n",
    "            },\n",
    "            is_best)\n",
    "\n",
    "        with open(args.save_path/args.log_summary, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t')\n",
    "            writer.writerow([train_loss, decisive_error])\n",
    "    logger.epoch_bar.finish()\n",
    "    \n",
    "    \n",
    "def train(args, train_loader, disp_net, pose_exp_net, optimizer, epoch_size, logger, tb_writer):\n",
    "    \n",
    "    global n_iter, device\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter(precision=4)\n",
    "    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    \n",
    "    end = time.time()\n",
    "    logger.train_bar.update(0)\n",
    "\n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(train_loader):\n",
    "        log_losses = i > 0 and n_iter % args.print_freq == 0\n",
    "        log_output = args.training_output_freq > 0 and n_iter % args.training_output_freq == 0\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "\n",
    "        # compute output\n",
    "        disparities = disp_net(tgt_img)\n",
    "        depth = [1/disp for disp in disparities]\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n",
    "\n",
    "        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs, intrinsics,\n",
    "                                                               depth, explainability_mask, pose,\n",
    "                                                               args.rotation_mode, args.padding_mode)\n",
    "        if w2 > 0:\n",
    "            loss_2 = explainability_loss(explainability_mask)\n",
    "        else:\n",
    "            loss_2 = 0\n",
    "        loss_3 = smooth_loss(depth)\n",
    "\n",
    "        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n",
    "\n",
    "        if log_losses:\n",
    "            tb_writer.add_scalar('photometric_error', loss_1.item(), n_iter)\n",
    "            if w2 > 0:\n",
    "                tb_writer.add_scalar('explanability_loss', loss_2.item(), n_iter)\n",
    "            tb_writer.add_scalar('disparity_smoothness_loss', loss_3.item(), n_iter)\n",
    "            tb_writer.add_scalar('total_loss', loss.item(), n_iter)\n",
    "\n",
    "        if log_output:\n",
    "            tb_writer.add_image('train Input', tensor2array(tgt_img[0]), n_iter)\n",
    "            for k, scaled_maps in enumerate(zip(depth, disparities, warped, diff, explainability_mask)):\n",
    "                log_output_tensorboard(tb_writer, \"train\", 0, \" {}\".format(k), n_iter, *scaled_maps)\n",
    "\n",
    "        # record loss and EPE\n",
    "        losses.update(loss.item(), args.batch_size)\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        with open(args.save_path/args.log_full, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter='\\t')\n",
    "            writer.writerow([loss.item(), loss_1.item(), loss_2.item() if w2 > 0 else 0, loss_3.item()])\n",
    "        logger.train_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.train_writer.write('Train: Time {} Data {} Loss {}'.format(batch_time, data_time, losses))\n",
    "        if i >= epoch_size - 1:\n",
    "            break\n",
    "\n",
    "        n_iter += 1\n",
    "\n",
    "    return losses.avg[0]\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_without_gt(args, val_loader, disp_net, pose_exp_net, epoch, logger, tb_writer, sample_nb_to_log=3):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter(i=3, precision=4)\n",
    "    log_outputs = sample_nb_to_log > 0\n",
    "    w1, w2, w3 = args.photo_loss_weight, args.mask_loss_weight, args.smooth_loss_weight\n",
    "    poses = np.zeros(((len(val_loader)-1) * args.batch_size * (args.sequence_length-1),6))\n",
    "    disp_values = np.zeros(((len(val_loader)-1) * args.batch_size * 3))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "    pose_exp_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    logger.valid_bar.update(0)\n",
    "    for i, (tgt_img, ref_imgs, intrinsics, intrinsics_inv) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        ref_imgs = [img.to(device) for img in ref_imgs]\n",
    "        intrinsics = intrinsics.to(device)\n",
    "        intrinsics_inv = intrinsics_inv.to(device)\n",
    "\n",
    "        # compute output\n",
    "        disp = disp_net(tgt_img)\n",
    "        depth = 1/disp\n",
    "        explainability_mask, pose = pose_exp_net(tgt_img, ref_imgs)\n",
    "\n",
    "        loss_1, warped, diff = photometric_reconstruction_loss(tgt_img, ref_imgs,\n",
    "                                                               intrinsics, depth,\n",
    "                                                               explainability_mask, pose,\n",
    "                                                               args.rotation_mode, args.padding_mode)\n",
    "        loss_1 = loss_1.item()\n",
    "        if w2 > 0:\n",
    "            loss_2 = explainability_loss(explainability_mask).item()\n",
    "        else:\n",
    "            loss_2 = 0\n",
    "        loss_3 = smooth_loss(depth).item()\n",
    "\n",
    "        if log_outputs and i < sample_nb_to_log - 1:  # log first output of first batches\n",
    "            if epoch == 0:\n",
    "                for j,ref in enumerate(ref_imgs):\n",
    "                    tb_writer.add_image('val Input {}/{}'.format(j, i), tensor2array(tgt_img[0]), 0)\n",
    "                    tb_writer.add_image('val Input {}/{}'.format(j, i), tensor2array(ref[0]), 1)\n",
    "\n",
    "            log_output_tensorboard(tb_writer, 'val', i, '', epoch, 1./disp, disp, warped[0], diff[0], explainability_mask)\n",
    "\n",
    "        if log_outputs and i < len(val_loader)-1:\n",
    "            step = args.batch_size*(args.sequence_length-1)\n",
    "            poses[i * step:(i+1) * step] = pose.cpu().view(-1,6).numpy()\n",
    "            step = args.batch_size * 3\n",
    "            disp_unraveled = disp.cpu().view(args.batch_size, -1)\n",
    "            disp_values[i * step:(i+1) * step] = torch.cat([disp_unraveled.min(-1)[0],\n",
    "                                                            disp_unraveled.median(-1)[0],\n",
    "                                                            disp_unraveled.max(-1)[0]]).numpy()\n",
    "\n",
    "        loss = w1*loss_1 + w2*loss_2 + w3*loss_3\n",
    "        losses.update([loss, loss_1, loss_2])\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        logger.valid_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.valid_writer.write('valid: Time {} Loss {}'.format(batch_time, losses))\n",
    "    if log_outputs:\n",
    "        prefix = 'valid poses'\n",
    "        coeffs_names = ['tx', 'ty', 'tz']\n",
    "        if args.rotation_mode == 'euler':\n",
    "            coeffs_names.extend(['rx', 'ry', 'rz'])\n",
    "        elif args.rotation_mode == 'quat':\n",
    "            coeffs_names.extend(['qx', 'qy', 'qz'])\n",
    "        for i in range(poses.shape[1]):\n",
    "            tb_writer.add_histogram('{} {}'.format(prefix, coeffs_names[i]), poses[:,i], epoch)\n",
    "        tb_writer.add_histogram('disp_values', disp_values, epoch)\n",
    "    logger.valid_bar.update(len(val_loader))\n",
    "    return losses.avg, ['Validation Total loss', 'Validation Photo loss', 'Validation Exp loss']\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_with_gt(args, val_loader, disp_net, epoch, logger, tb_writer, sample_nb_to_log=3):\n",
    "    global device\n",
    "    batch_time = AverageMeter()\n",
    "    error_names = ['abs_diff', 'abs_rel', 'sq_rel', 'a1', 'a2', 'a3']\n",
    "    errors = AverageMeter(i=len(error_names))\n",
    "    log_outputs = sample_nb_to_log > 0\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    disp_net.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    logger.valid_bar.update(0)\n",
    "    for i, (tgt_img, depth) in enumerate(val_loader):\n",
    "        tgt_img = tgt_img.to(device)\n",
    "        depth = depth.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output_disp = disp_net(tgt_img)\n",
    "        output_depth = 1/output_disp[:,0]\n",
    "\n",
    "        if log_outputs and i < sample_nb_to_log:\n",
    "            if epoch == 0:\n",
    "                tb_writer.add_image('val Input/{}'.format(i), tensor2array(tgt_img[0]), 0)\n",
    "                depth_to_show = depth[0]\n",
    "                tb_writer.add_image('val target Depth Normalized/{}'.format(i),\n",
    "                                    tensor2array(depth_to_show, max_value=None),\n",
    "                                    epoch)\n",
    "                depth_to_show[depth_to_show == 0] = 1000\n",
    "                disp_to_show = (1/depth_to_show).clamp(0,10)\n",
    "                tb_writer.add_image('val target Disparity Normalized/{}'.format(i),\n",
    "                                    tensor2array(disp_to_show, max_value=None, colormap='magma'),\n",
    "                                    epoch)\n",
    "\n",
    "            tb_writer.add_image('val Dispnet Output Normalized/{}'.format(i),\n",
    "                                tensor2array(output_disp[0], max_value=None, colormap='magma'),\n",
    "                                epoch)\n",
    "            tb_writer.add_image('val Depth Output Normalized/{}'.format(i),\n",
    "                                tensor2array(output_depth[0], max_value=None),\n",
    "                                epoch)\n",
    "\n",
    "        errors.update(compute_errors(depth, output_depth))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        logger.valid_bar.update(i+1)\n",
    "        if i % args.print_freq == 0:\n",
    "            logger.valid_writer.write('valid: Time {} Abs Error {:.4f} ({:.4f})'.format(batch_time, errors.val[0], errors.avg[0]))\n",
    "    logger.valid_bar.update(len(val_loader))\n",
    "    return errors.avg, error_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
