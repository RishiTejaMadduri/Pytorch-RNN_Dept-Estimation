{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data.data_loader import *\n",
    "# from model import *\n",
    "# import time\n",
    "# from utils_lr import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_depth_trainer:\n",
    "    def initDataloader(self, dataset_dir, batch_size, img_height, img_widht, num_views, num_epochs, is_training):\n",
    "        \n",
    "        self.img_height = img_height\n",
    "        self.img_width = img_width\n",
    "        self.batch_size = batch_size\n",
    "        self.num_views = num_views\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        initloader=DataLoader(dataset_dir, batch_size, img_height, img_width, num_epochs, self.num_views)\n",
    "        \n",
    "        dataLoader=initloader.inputs(is_training)\n",
    "        \n",
    "        return dataLoader\n",
    "    \n",
    "    def load_data(self, dataLoader):\n",
    "        dataiter = iter(dataLoader)  #this could be an issue\n",
    "        data_dict = dataiter.next()\n",
    "\n",
    "        return data_dict\n",
    "    \n",
    "    def construct_model(self, data_dict):\n",
    "    #Forward    \n",
    "        image_seq=data_dict['image_seq']\n",
    "        hidden_state = [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
    "        hidden_state_pos = [None, None, None, None, None, None, None]\n",
    "        est_poses = []\n",
    "        \n",
    "        for i in range(self.num_views):\n",
    "            \n",
    "            image=image_seq[0:-1, 0:-1, self.img_width*i: int(self.img_width), 0:-1]\n",
    "            image=torch.reshape(image,([self.batch_size, self.image_height,self.img_widht,3]))\n",
    "            \n",
    "            pred_depth, hidden_state = rnn_depth_net_encoderlstm()\n",
    "            pred_pose, hidden_state_pose = pose_net(torch.cat((image, pred_depth),3), hidden_state_pose, is_training=True)\n",
    "            est_poses.append(pred_pose)\n",
    "            \n",
    "            if i==0:\n",
    "                est_depths=pred_depth\n",
    "                \n",
    "            else:\n",
    "                est_depths = torch.cat((est_depths, pred_depths),2)\n",
    "                \n",
    "    #Backward\n",
    "        \n",
    "        image_seq=data_dict['image_seq']\n",
    "        hidden_state = [None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None]\n",
    "        hidden_state_pose= [None,None,None,None,None,None,None]\n",
    "        est_poses_bw = []\n",
    "        \n",
    "        for i in range(self.num_views-1, -1, -1):\n",
    "            \n",
    "            image=image_seq[0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "            depth=data_dict['depth_seq'][0:-1, 0:-1, self.img_widht*i:int(self.img_width), 0:-1]\n",
    "            \n",
    "            image=torch.reshape(image, [self.batch_size, self.img_height, self.img_width,3])\n",
    "            \n",
    "            pred_depth, hidden_state = rnn_depth_net_encoderlstm(image, hidden_state,is_training=True)\n",
    "            pred_pose, hidden_state_pose = pose_net(torch.concat((image,pred_depth),3), hidden_state_pose,is_training=True)\n",
    "            \n",
    "            est_poses_bw.append(pred_pose)\n",
    "            \n",
    "            if i==self.num_views-1:\n",
    "                est_depths_bw=pred_depth\n",
    "                depth_seq_bw=depth\n",
    "            else:\n",
    "                est_depth_bw = torch.cat((est_depths_bw, pred_depth), 2)\n",
    "                depth_seq_bw = torch.cat((depth_seq_bw, depth),2)\n",
    "                \n",
    "        data_dict['depth_seq_bw'] = depth_seq_bw\n",
    "        \n",
    "        return [est_depths, est_poses, est_depths_bw, est_poses_bw]\n",
    "    \n",
    "    \n",
    "    def compute_losses(self, estimates, data_dict, global_step):\n",
    "        est_depths = estimates[0]\n",
    "        est_poses = estimates[1]\n",
    "        est_depths_bw=estimates[2]\n",
    "        est_depths_bw=estimates[3]\n",
    "        \n",
    "        all_losses = []\n",
    "        output_dict = {}\n",
    "        \n",
    "        diff = torch.abs(label-pred)\n",
    "        diff = torch.where(torch.isinf(diff), torch.zeros_like(diff), diff)\n",
    "        diff = torch.where(torch.isnan(diff), torch.zeros_like(diff), diff)\n",
    "        dif  = torch.count_nonzero(diff).float32\n",
    "        \n",
    "        if v_weight is not None:\n",
    "            diff = torch.mul(diff, v_weight) #Elemetn Wise Product\n",
    "            \n",
    "        if v_weight is not None:\n",
    "            return torch.sum(diff)/torch.count_nonzero(v_weight).float32\n",
    "        \n",
    "        else:\n",
    "            return torch.sum(diff)/div\n",
    "        \n",
    "        def smooth_l1_loss(y_true, y_pred, v_weight = None):\n",
    "            \"\"\"Implements Smooth L1 Loss\n",
    "            y_true and y_pred are typically: [N, 4], but could be any shape.\n",
    "            \"\"\"\n",
    "            diff = torch.abs(y_true - y_pred)\n",
    "            diff = torch.where(torch.isinf(diff), torch.zeros_like(diff), diff)\n",
    "            diff = torch.where(torch.isnan(diff), torch.zeros_like(diff), diff)\n",
    "\n",
    "            if v_wieght is not None:\n",
    "                diff = torch.mul(diff, v_weight)\n",
    "            less_than_one = torch.gt(diff,1.0).float32\n",
    "            loss = (less_than_one * 0.5 * diff**2) + (1 - less_than_one) * (diff - 0.5)\n",
    "\n",
    "            if v_weight is not None:\n",
    "                return torch.sum(loss)/torch.count_nonzero(v_weight).float32\n",
    "            else:\n",
    "                return torch.mean(loss)\n",
    "\n",
    "        def gradient(pred, delta):\n",
    "            D_dx = pred[:, delta:, :, :] - pred[:, :-delta, :, :]\n",
    "            D_dy = pred[:, :, delta:, :] - pred[:, :, :-delta, :]\n",
    "\n",
    "            return D_dx, D_dy\n",
    "\n",
    "        def compute_smooth_loss(img, disp, delta):\n",
    "            disp_gradients_x, disp_gradients_y = gradient(disp, delta)\n",
    "            image_gradients_x, image_gradients_y = gradient(img, delta)\n",
    "\n",
    "            weights_x = torch.exp(-torch.mean(torch.abs(image_gradients_x), 3))\n",
    "            weights_y = torch.exp(-torch.mean(torch.abs(image_gradients_y), 3))\n",
    "\n",
    "            smoothness_x = disp_gradients_x * weights_x\n",
    "            smoothness_y = disp_gradients_y * weights_y\n",
    "\n",
    "            return torch.mean(torch.abs(smoothness_x)) + torch.mean(torch.abs(smoothness_y))\n",
    "\n",
    "        def SSIM(x,y):\n",
    "            C1 = 0.01 ** 2\n",
    "            C2 = 0.03 ** 2\n",
    "\n",
    "            mu_x = F.avg_pool2d(x, 3, 1, 'SAME')\n",
    "            mu_y = F.avg_pool2d(y, 3, 1, 'SAME')\n",
    "\n",
    "            sigma_x = F.avg_pool2d(x ** 2, 3,1 , 'SAME') - mu_x ** 2\n",
    "            sigma_y = F.avg_pool2d(y ** 2, 3,1 , 'SAME') - mu_y ** 2\n",
    "            sigma_xy = F.avg_pool2d(x * y , 3, 1, 'SAME') - mu_x * mu_y\n",
    "\n",
    "            SSIM_n = (2 * mu_x * mu_y + C1) * (2 * sigma_xy + C2)\n",
    "            SSIM_d = (mu_x ** 2 + mu_y ** 2 + C1) * (sigma_x + sigma_y + C2)\n",
    "\n",
    "            SSIM = SSIM_n / SSIM_d\n",
    "\n",
    "            return torch.clamp((1-SSIM)/2, 0, 1)\n",
    "\n",
    "        #Write This Function\n",
    "        #def image_similarity(x, y, v_weight = None, alpha=1.0):\n",
    "        #ss\n",
    "\n",
    "        # Direct inverse depth loss\n",
    "        depth_loss_fw = torch.zeros([])\n",
    "        gt_depth_fw = 1/data_dict['depth_seq']\n",
    "        depth_loss_fw = l1loss(est_depths, gt_depth_fw)\n",
    "        all_losses.append(depth_loss_fw*10)\n",
    "\n",
    "        output_dict['depth'] = est_depths\n",
    "\n",
    "        #Backward depth\n",
    "        depth_loss_bw = torch.zeros([]).float32\n",
    "        gt_depth_bw = 1.0/data_dict['depth_seq_bw']\n",
    "        depth_loss_bw = l1loss(est_depths_bw, gt_depth_bw)\n",
    "        all_losses.append(depth_loss_bw*10)\n",
    "\n",
    "        output_dict['depth_bw'] = est_depths_bw\n",
    "\n",
    "        #Gradient Loss\n",
    "\n",
    "        gradient_loss_fw=torch.zeros([]).float32\n",
    "\n",
    "        for i in range(self.num_views):\n",
    "            depth_slice = est_depths[0:-1, 0:-1, (self.img_width*i):int(self.img_width), 0:-1]\n",
    "\n",
    "            gt_depth_slice = gt_depth_fw[0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "            image_slice = data_dict['image_seq'][0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "            Dx1, Dy1 = gradient(depth_slice,1); gtDx1, gtDy1 = gradient(gt_depth_slice,1)\n",
    "            Dx2, Dy2 = gradient(depth_slice,2); gtDx2, gtDy2 = gradient(gt_depth_slice,2)\n",
    "            Dx4, Dy4 = gradient(depth_slice,4); gtDx4, gtDy4 = gradient(gt_depth_slice,4)\n",
    "            Dx8, Dy8 = gradient(depth_slice,8); gtDx8, gtDy8 = gradient(gt_depth_slice,8)\n",
    "\n",
    "            gradient_loss_fw += (l1loss(Dx1, gtDx1)+\n",
    "                               l1loss(Dy1, gtDy1))\n",
    "            gradient_loss_fw += (l1loss(Dx2, gtDx2)+\n",
    "                           l1loss(Dy2, gtDy2))\n",
    "            gradient_loss_fw += (l1loss(Dx4, gtDx4)+\n",
    "                           l1loss(Dy4, gtDy4))\n",
    "            gradient_loss_fw += (l1loss(Dx8, gtDx8)+\n",
    "                           l1loss(Dy8, gtDy8))\n",
    "\n",
    "        all_losses.append(gradient_loss_fw*20)\n",
    "\n",
    "        #Backward Gradient\n",
    "        gradient_loss_bw = torch.zeros([]).float32\n",
    "\n",
    "        for i in range(self.num_views):\n",
    "\n",
    "            depth_slice=est_depths_bw[0:-1, 0:-1, self.img_width*i:int(img_width), 0:-1]\n",
    "            gt_depth_slice = gt_depth_bw[0:-1, 0:-1, self.img_width*i:int(img_width), 0:-1]\n",
    "            image_slice = data_dict['image_seq'][0:-1, 0:-1, self.img_width*(self.num_views-i-1):int(self.img_width), 0:-1]\n",
    "\n",
    "            Dx1, Dy1 = gradient(depth_slice,1); gtDx1, gtDy1 = gradient(gt_depth_slice,1)\n",
    "            Dx2, Dy2 = gradient(depth_slice,2); gtDx2, gtDy2 = gradient(gt_depth_slice,2)\n",
    "            Dx4, Dy4 = gradient(depth_slice,4); gtDx4, gtDy4 = gradient(gt_depth_slice,4)\n",
    "            Dx8, Dy8 = gradient(depth_slice,8); gtDx8, gtDy8 = gradient(gt_depth_slice,8)\n",
    "\n",
    "            gradient_loss_bw += (l1loss(Dx1, gtDx1)+\n",
    "                               l1loss(Dy1, gtDy1))\n",
    "            gradient_loss_bw += (l1loss(Dx2, gtDx2)+\n",
    "                           l1loss(Dy2, gtDy2))\n",
    "            gradient_loss_bw += (l1loss(Dx4, gtDx4)+\n",
    "                           l1loss(Dy4, gtDy4))\n",
    "            gradient_loss_bw += (l1loss(Dx8, gtDx8)+\n",
    "                               l1loss(Dy8, gtDy8))\n",
    "\n",
    "        all_losses.append(gradient_loss_bw*20)\n",
    "\n",
    "        #Pose Loss-Image reconstruction\n",
    "        pose_loss_fw = torch.zeros([]).float32\n",
    "        mask_loss_fw = torch.zeros([]).float32\n",
    "\n",
    "\n",
    "        mask_regular = torch.ones((self.batch_size, self.img_height, self.img_width, 1))\n",
    "\n",
    "        #Backward pose\n",
    "        pose_loss_bw = torch.zeros([]).float32\n",
    "        mask_loss_bw = torch.zeros([]).float32\n",
    "\n",
    "\n",
    "        consist_loss_fw = torch.zeros([]).float32\n",
    "        consist_loss_bw = torch.zeros([]).float32\n",
    "\n",
    "        # Multi-view image reprojection loss\n",
    "        for i in range(self.num_views):\n",
    "\n",
    "            if i==0:\n",
    "\n",
    "                depth_slice_fw = est_depths[0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "                depth_slice_bw = est_depths_bw[0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "                image_slice_fw = data_dict['image_seq'][0:-1, 0:-1, self.img_width*i: int(self.img_width), 0:-1]\n",
    "\n",
    "                image_slice_bw = data_dict['image_seq'][0:-1, 0:-1, self.img_width*(self.num_views-1): int(self.img_width), 0:-1]\n",
    "\n",
    "                depth_slice_fw.reshape([self.batch_size, self.img_height, self.img_width, 1])\n",
    "                depth_slice_bw.reshape([self.batch_size, self.img_height, self.img_width, 1])\n",
    "                image_slice_fw.reshape([self.batch_size, self.img_height, self.img_width, 3])\n",
    "                image_slice_bw.reshape([self.batch_size, self.img_height, self.img_width, 3])\n",
    "\n",
    "                proj_img, wmask, flow_fw = projective_inverse_warp(image_slice_fw, torch.squeeze( 1.0/depth_slice_fw, axis=3), est_poses[i], data_dict['intrinsics'], format='eular')\n",
    "\n",
    "                pose_loss_fw += image_similarity(image_slice_fw, proj_img, wmask)*0.1 #Shouldnt work\n",
    "                mask_loss_fw +=l1loss(mask_regular, wmask)*0.1\n",
    "\n",
    "                proj_img, wmask, flow_bw = projective_inverse_warp(image_slice_bw, torch.squeeze( (1.0/depth_slice_bw), 3), est_poses_bw[i], data_dict['intrinsics'], format='eular')\n",
    "                pose_loss_bw += image_similarity(image_slice_bw, proj_img, wmask)*0.1 #shouldnt work\n",
    "                mask_loss_bw += l1loss(mask_regular, wmask)*0.1\n",
    "\n",
    "                #Visualize first depth map\n",
    "                if i==0:\n",
    "                depth_slice_fw1 = depth_slice_fw\n",
    "                continue\n",
    "\n",
    "            # Get current image and depth\n",
    "            depth_slice_bw = est_depths[0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "            image_slice_fw = data_dict['image_seq'][0:-1, 0:-1, self.img_width*i:int(self.img_width), 0:-1]\n",
    "\n",
    "            depth_slice_fw = torch.reshape(depth_slice_fw, (self.batch_size, self.img_height, self.img_width, 1))\n",
    "            image_slice_fw = torch.reshape(image_slice_fw, (self.batch_size, self.img_height, self.img_width, 3))\n",
    "\n",
    "            accum_pose_fw = torch.eye(4)\n",
    "            accum_pose_fw = accum_pose_fw.rshape((1,4,4))\n",
    "            accum_pose_fw = accum_pose_fw.repeat(self.batch_size, 1, 1)\n",
    "\n",
    "            accum_pose_bw = torch.eye(4)\n",
    "            accum_pose_bw = accum_pose_bw.rshape((1,4,4))\n",
    "            accum_pose_bw = accum_pose_bw.repeat(self.batch_size, 1, 1)\n",
    "\n",
    "            if i==4:\n",
    "                depth_slice_fw5 = depth_slice_fw\n",
    "\n",
    "            degrade=0\n",
    "\n",
    "            # Project current view into every previous view using accumulate\n",
    "            # transformation\n",
    "            \n",
    "            for j in range(i-1, -1, -1):\n",
    "                \n",
    "                previous_scene_fw = data_dict['image_seq'][0:-1, 0: -1, self.img_width * j:int(self.img_width), 0:-1]\n",
    "                previous_scene_fw = previous_scene_fw.reshape(self.batch_size, self.img_height, self. img_width, 3)\n",
    "                accum_pose_fw = torch.mm(pose_vec2mat(est_poses[j+1], 'euler'), accum_pose_fw) #Matrix multiplication\n",
    "\n",
    "                # Differentiable geometric module (DGM)\n",
    "                # Using depth and pose to compute warped image, and warping flow\n",
    "\n",
    "                proj_img_fw, wmask_fw, flow_fw = projective_inverse_warp(previous_scene_fw, torch.squeeze( (1.0/depth_slice_fw), 3),accum_pose_fw,data_dict['intrinsics'],format='matrix')\n",
    "\n",
    "                \n",
    "                # Perform the same operation for backward prediction\n",
    "                depth_slice_bw = est_depths_bw[0:-1, 0: -1, self.img_width * (self.num_views-j-1):int(self.img_width), 0:-1]\n",
    "                depth_slice_bw = depth_slice_bw.reshape(self.batch_size, self.img_height, self. img_width, 1)\n",
    "                accum_pose_bw  = torch.mm(accum_pose_bw, pose_vec2mat(est_poses_bw[self.num_views-j-1],'eular'))\n",
    "                \n",
    "                proj_img_bw, wmask_bw, flow_bw = projective_inverse_warp(image_slice_fw, torch.squeeze( 1.0/depth_slice_bw, dim=3), accum_pose_bw, data_dict['intrinsics'], format='matrix')\n",
    "                \n",
    "                \n",
    "                # Compute Forward backward flow\n",
    "                Fba_i, _, _ = projective_inverse_warp(-flow_bw, torch.squeeze(1.0/depth_slice_fw, 3), accum_pose_fw, data_dict['intrinsics'], format='matrix')\n",
    "\n",
    "                Fab_i, _, _ = projective_inverse_warp(-flow_fw, torch.squeeze(1.0 / depth_slice_bw, axis=3), accum_pose_bw, data_dict['intrinsics'],format='matrix')\n",
    "\n",
    "                #Find inconsistency\n",
    "                fw_flowdiff = torch.norm(flow_fw - Fba_i, dim=3)\n",
    "                bw_flowdiff = torch.norm(flow_bw - Fab_i, dim=3)\n",
    "\n",
    "                #A Threshold to filter out moving objects or occlusion boundaries\n",
    "                fw_threshold_flow = torch.where((0.05*torch.norm(flow_fw, dim=3)<=3.0), torch.ones_like(fw_flowdiff)*3.0, 0.05*torch.norm(flow_fw, dim=3)) #Recheck\n",
    "                bw_threshold_flow = torch.where((0.05*torch.norm(flow_bw, dim=3)<=3.0), torch.ones_like(fw_flowdiff)*3.0, 0.05*torch.norm(flow_bw, dim=3)) #Recheck\n",
    "\n",
    "                fw_occ_mask = torch.unsqueeze((torch.where(fw_flowdiff<=fw_threshold_flow), torch.ones_like(fw_flowdiff), torch.zeros_like(fw_flowdiff)),axis=-1)\n",
    "                bw_occ_mask = torch.unsqueeze((torch.where(bw_flowdiff<=bw_threshold_flow), torch.ones_like(fw_flowdiff), torch.zeros_like(fw_flowdiff)),axis=-1)\n",
    "\n",
    "                #A mask for valid pixels.\n",
    "                wmask_fw = wmask_fw*fw_occ_mask\n",
    "                wmask_bw = wmask_bw*bw_occ_mask\n",
    "\n",
    "                # Image reprojection loss\n",
    "                pose_loss_fw += image_similarity(image_slice_fw, proj_img_fw, wmask_fw)/2**degrade/10.0 #Function doesn't exist, wont work\n",
    "                pose_loss_bw += image_similarity(previous_scene_fw, proj_img_bw, wmask_bw)/2**degrade/10.0 #Function doesn't exist, wont work\n",
    "\n",
    "\n",
    "                # Foward back flow consistency loss\n",
    "                if j == i-1:\n",
    "                    consist_fw_err = l1loss(flow_fw, Fba_i, wmask_fw[:,:,:,0:2])\n",
    "                    consist_loss_fw += consist_fw_err*0.01/2**degrade\n",
    "                    consist_bw_err = l1loss(flow_bw, Fab_i, wmask_bw[:,:,:,0:2])\n",
    "                    consist_loss_bw += consist_bw_err*0.01/2**degrade\n",
    "\n",
    "                    # A regularization loss on mask to prevent trivial solution\n",
    "                    mask_loss_fw +=l1loss(mask_regular, wmask_fw)*0.01/2**degrade\n",
    "                    mask_loss_bw +=l1loss(mask_regular, wmask_bw)*0.01/2**degrade\n",
    "\n",
    "                    flow_fw_img = flow_fw\n",
    "\n",
    "                if i==1:\n",
    "                    flow_bw_img = flow_bw\n",
    "\n",
    "                degrade+=1\n",
    "\n",
    "        all_losses.append(pose_loss_fw)\n",
    "        all_losses.append(pose_loss_bw)\n",
    "        data_dict['est_pose'] = est_poses\n",
    "        data_dict['est_pose_bw'] = est_poses_bw\n",
    "        \n",
    "        all_losses.append(mask_loss_fw)\n",
    "        all_losses.append(mask_loss_bw)\n",
    "        all_losses.append(consist_loss_fw)\n",
    "        all_losses.append(consist_loss_bw)\n",
    "        \n",
    "        total_loss = tf.reduce_mean(all_losses)\n",
    "\n",
    "        output_dict['previous_scene_fw'] = previous_scene_fw\n",
    "        output_dict['proj_img_fw'] = proj_img_fw\n",
    "        output_dict['image_slice_fw'] = image_slice_fw\n",
    "        output_dict['depth_slice_fw'] = depth_slice_fw\n",
    "\n",
    "        output_dict['previous_scene_bw'] = image_slice_fw\n",
    "        output_dict['proj_img_bw'] = proj_img_bw\n",
    "        output_dict['image_slice_bw'] = previous_scene_fw\n",
    "        output_dict['depth_slice_bw'] = depth_slice_bw\n",
    "\n",
    "        output_dict['flow_bw'] = flow_bw_img\n",
    "        output_dict['flow_fw'] = flow_fw_img\n",
    "\n",
    "        data_dict['flow_bw'] = flow_bw\n",
    "        data_dict['flow_fw'] = flow_fw\n",
    "\n",
    "        output_dict['depth_slice_fw1'] = depth_slice_fw1\n",
    "        output_dict['depth_slice_fw5'] = depth_slice_fw5\n",
    "\n",
    "        return total_loss, all_losses, output_dict\n",
    "\n",
    "        \n",
    "    #Write Summary Funcitons\n",
    "    def sub_depth(self, est_depths):\n",
    "        \n",
    "        depth_slice1 = est_depths[0:-1, 0:-1, self.img_width*0:int(self.img_width), 0:-1]\n",
    "        depth_slice2 = est_depths[0:-1, 0:-1, self.img_width*5:int(self.img_width), 0:-1]\n",
    "        depth_slice3 = est_depths[0:-1, 0:-1, self.img_width*9:int(self.img_width), 0:-1]\n",
    "        depth=1.0/torch.cat((depth_slice1, depth_slice2, depth_slice3), 2)\n",
    "        return depth\n",
    "    \n",
    "        \n",
    "#     def tb_summary(self, output_dict, loss, all_losses):\n",
    "        \n",
    "#         color_depth_fw = torch.unsqueeze(depth_plasma(output_dict['depth_slice_fw']), 0)\n",
    "        \n",
    "    #Run Session\n",
    "#     def save(self, sess, checkpoint_dir, step, saver):\n",
    "#         '''\n",
    "#         Save Checkpoints\n",
    "#         '''\n",
    "#         model_name = 'model'\n",
    "        \n",
    "#         if step == 'latest':\n",
    "#             saver.save(sess,\n",
    "#                        os.path.join(checkpoint_dir, model_name + '.latest'))\n",
    "#         else:\n",
    "#             saver.save(sess, os.path.join(checkpoint_dir, model_name), global_step=step)\n",
    "\n",
    "            \n",
    "    #Write train function\n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
