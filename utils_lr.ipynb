{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_color_depth(depth,filename):\n",
    "    plt.imsave(filename, depth, cmap= 'plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray2rgb(im, cmap='gray'):\n",
    "    cmap=plt.get_cmap(cmap)\n",
    "    rgba_img=cmap(im.astype(np.float32))\n",
    "    rgb_img=np.delete(rgba_img,3,2)\n",
    "    return rgb_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_depth_for_display(depth, pc=95, crop_percent=0, normalize=None, cmap='gray'):\n",
    "    \n",
    "    #Convert to disparity\n",
    "    depth=1./(depth+1e-6)\n",
    "    \n",
    "    if normalizer is not None:\n",
    "        depth=depth/normalizer\n",
    "    else:\n",
    "        depth=depth/(np.percentile(depth, pc)+1e-6)\n",
    "        \n",
    "    depth = np.clip(depth,0,1)\n",
    "    depth = gray2rgb(depth, cmap=cmap)\n",
    "    keep_H = int(depth.shape[0]*(1-crop_percent))\n",
    "    depth = depth[:keep_H]\n",
    "    depth = depth\n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_gradient_direction(img):\n",
    "    eps=1e-11\n",
    "    sobelx=cv.Sobel(img, cv.CV_64F,1,1,ksize=5)+eps\n",
    "    sobely=cv.Sobel(img, cv.CV_64F,0,1,ksize=5)+eps\n",
    "    \n",
    "    direction=np.arctan(sobely/sobelx)*180/np.pi\n",
    "    direction[(direction<=0) & (sobelx<0)]+=180\n",
    "    direction[(direction<0) & (sobely<0)]+=360\n",
    "    direction[(direction>0) & (sobelx<0)]+=180\n",
    "    \n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original-Rewritten\n",
    "def euler2mat(z,y,x):\n",
    "    \"\"\"Converts euler angles to rotation matrix\n",
    "       TODO: remove the dimension for 'N' (deprecated for converting all source\n",
    "             poses altogether)\n",
    "       Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n",
    "      Args:\n",
    "          z: rotation angle along z axis (in radians) -- size = [B, N]\n",
    "          y: rotation angle along y axis (in radians) -- size = [B, N]\n",
    "          x: rotation angle along x axis (in radians) -- size = [B, N]\n",
    "      Returns:\n",
    "          Rotation matrix corresponding to the euler angles -- size = [B, N, 3, 3]\n",
    "      \"\"\"\n",
    "    \n",
    "    B = z.size(0)\n",
    "    N = 1\n",
    "    z = torch.unsqueeze(torch.unsqueeze(z,-1), -1)\n",
    "    y = torch.unsqueeze(torch.unsqueeze(y,-1), -1)\n",
    "    x = torch.unsqueeze(torch.unsqueeze(x,-1), -1)\n",
    "    \n",
    "    zeros = torch.zeros((B,N,1,1))\n",
    "    ones = torch.ones((B,N,1,1))\n",
    "    \n",
    "    cosz = torch.cos(z)\n",
    "    sinz = torch.sin(z)\n",
    "    \n",
    "    rotz_1 = torch.cat([cosz, -sinz, zeros], axis=3)\n",
    "    rotz_2 = torch.cat([cosz, -sinz, zeros], axis=3)\n",
    "    rotz_3 = torch.cat([cosz, -sinz, zeros], axis=3)\n",
    "    zmat =  torch.cat([rotz_1, rotz_2, rotz_3], axis=2)\n",
    "    \n",
    "    cosy = torch.cos(y)\n",
    "    siny = torch.sin(y)\n",
    "    \n",
    "    roty_1 = torch.cat([cosy, zeros, siny], axis=3)\n",
    "    roty_2 = torch.cat([zeros, ones, zeros], axis=3)\n",
    "    roty_3 = torch.cat([-siny,zeros, cosy], axis=3)\n",
    "    ymat = torch.cat([roty_1, roty_2, roty_3], axis=2)\n",
    "    \n",
    "    cosx = torch.cos(x)\n",
    "    sinx = torch.sin(x)\n",
    "    \n",
    "    rotx_1 = torch.cat([ones, zeros, zeros], axis=3)\n",
    "    rotx_2 = torch.cat([zeros, cosx, -sinx], axis=3)\n",
    "    rotx_3 = torch.cat([zeros, sinx, cosx], axis=3)\n",
    "    xmat = torch.cat([rotx_1, rotx_2, rotx_3], axis=2) \n",
    "    \n",
    "    \n",
    "    rotMat = torch.mat(torch.mat(xmat, ymat), zmat)\n",
    "    \n",
    "    return rotmat\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SFM Learner-Change accordingly\n",
    "# def euler2mat(angle):\n",
    "#     \"\"\"Convert euler angles to rotation matrix.\n",
    "#      Reference: https://github.com/pulkitag/pycaffe-utils/blob/master/rot_utils.py#L174\n",
    "#     Args:\n",
    "#         angle: rotation angle along 3 axis (in radians) -- size = [B, 3]\n",
    "#     Returns:\n",
    "#         Rotation matrix corresponding to the euler angles -- size = [B, 3, 3]    \n",
    "#     \"\"\"\n",
    "#     B = angle.size(0)\n",
    "#     x, y, z = angle[:,0], angle[:,1], angle[:,2]\n",
    "\n",
    "#     cosz = torch.cos(z)\n",
    "#     sinz = torch.sin(z)\n",
    "\n",
    "#     zeros = z.detach()*0\n",
    "#     ones = zeros.detach()+1\n",
    "#     zmat = torch.stack([cosz, -sinz, zeros,\n",
    "#                         sinz,  cosz, zeros,\n",
    "#                         zeros, zeros,  ones], dim=1).reshape(B, 3, 3)\n",
    "\n",
    "#     cosy = torch.cos(y)\n",
    "#     siny = torch.sin(y)\n",
    "\n",
    "#     ymat = torch.stack([cosy, zeros,  siny,\n",
    "#                         zeros,  ones, zeros,\n",
    "#                         -siny, zeros,  cosy], dim=1).reshape(B, 3, 3)\n",
    "\n",
    "#     cosx = torch.cos(x)\n",
    "#     sinx = torch.sin(x)\n",
    "\n",
    "#     xmat = torch.stack([ones, zeros, zeros,\n",
    "#                         zeros,  cosx, -sinx,\n",
    "#                         zeros,  sinx,  cosx], dim=1).reshape(B, 3, 3)\n",
    "\n",
    "#     rotMat = xmat @ ymat @ zmat\n",
    "#     return rotMat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rot2euler(R):\n",
    "#     sy = torch.sqrt(R[:,0,0] * R[:,0,0] +  R[:,1,0] * R[:,1,0])\n",
    "#     eps = torch.tensor(1e-6, shape=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Accordingly-May not need this\n",
    "def axis_angle_to_rotation_matrix(axis,angle):\n",
    "    B=angle.size(0)\n",
    "    z=angle[:,2]\n",
    "    zeros=z.detach()*0\n",
    "    ones = zeros.detach()+1\n",
    "    \n",
    "    Mat1=torch.cat([zeros,-torch.expand_dims(torch.expand_dims(axis[:,2],-1),-1), torch.expand_dims(torch.expand_dims(axis[:,1],-1),-1)], axis=2)\n",
    "    Mat2=torch.cat([zeros, zeros, -torch.expand_dims(torch.expand_dims(axis[:,0],-1),-1)],axis=2)\n",
    "    Mat3=torch.cat([zeros,zeros,zeros],axis=2)\n",
    "    \n",
    "    Mat=torch.cat([Mat1,Mat2,Mat3],axis=1)\n",
    "    \n",
    "    cp_axis=Mat-torch.transpose(Mat,perm=[0,2,1])\n",
    "    \n",
    "    RotMat=torch.eye(3, batch_shape=[B])+torch.sin(angle)*cp_axis+(ones-torch.cose(angle))*torch.matmaul(cp_axis, cp_axis)\n",
    "    \n",
    "    return RotMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original-Rewritten\n",
    "def pose_vec2mat(vec,format):\n",
    "    \n",
    "    batch_size = list(vec.shape())\n",
    "    translation = vec[:, :3]\n",
    "    translation =  torch.unsqueeze(-1)\n",
    "    \n",
    "    if format == 'euler':\n",
    "        rx = vec[:, 3:4]\n",
    "        ry = vec[:, 4:5]\n",
    "        rz = vec[:, 5:6]\n",
    "        rot_mat = euler2mat(rz, ry, rx)\n",
    "        rot_mat = torch.squeeze(1)\n",
    "        \n",
    "    elif format == 'angleaxis':\n",
    "        axis = vec[:, 3:]\n",
    "        angle = torch.unsqueeze(torch.norm(axis, axis=1), -1)\n",
    "        axis = axis/angle\n",
    "        angle = torch.unsqueeze(-1)\n",
    "        rot_mat = axis_angle_to_rotation_matrix(axis, angle)\n",
    "        \n",
    "    filler = torch.tensor([0,0,0,1]).reshape(1,1,4)\n",
    "    filler = torch.repeat_interleave(filler, repeats = batch_size, dim=0)\n",
    "    transform_mat = torch.cat([rot_mat, translation], axis = 2)\n",
    "    transform_mat = torch.cat([transform_mat, filler], axis =1)\n",
    "    \n",
    "    return transform_mat\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Modify according to axis_angle_to_rotation_matrix -Probably need changing\n",
    "# def pose_vec2mat(vec, format):\n",
    "#     \"\"\"\n",
    "#     Convert 6DoF parameters to transformation matrix.\n",
    "#     Args:s\n",
    "#         vec: 6DoF parameters in the order of tx, ty, tz, rx, ry, rz -- [B, 6]\n",
    "#     Returns:\n",
    "#         A transformation matrix -- [B, 3, 4]\n",
    "#     \"\"\"\n",
    "#     translation = vec[:, :3].unsqueeze(-1)  # [B, 3, 1]\n",
    "#     rot = vec[:,3:]\n",
    "# #     filler=torch.tensor([[0, 0, 0, 1]]).type(dtype)\n",
    "#     if format == 'euler':\n",
    "#         rot_mat = euler2mat(rot)  # [B, 3, 3]\n",
    "    \n",
    "#     elif format == 'angleaxis':\n",
    "#         angle=torch.expand_dims(torch.norm(rot, dim=1),-1)\n",
    "#         rot_mat = axis_angle_to_rotation_matrix(rot,angle)  # [B, 3, 3]\n",
    "#     transform_mat = torch.cat([rot_mat, translation], dim=2)  # [B, 3, 4]\n",
    "#     return transform_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Original-Rewritten\n",
    "def pixel2cam(depth, pixel_coords, intrinsics, is_homogeneous=True):\n",
    "    \"\"\"Transforms coordinates in the pixel frame to the camera frame.\n",
    "\n",
    "        Args:\n",
    "        depth: [batch, height, width]\n",
    "        pixel_coords: homogeneous pixel coordinates [batch, 3, height, width]\n",
    "        intrinsics: camera intrinsics [batch, 3, 3]\n",
    "        is_homogeneous: return in homogeneous coordinates\n",
    "        Returns:\n",
    "        Coords in the camera frame [batch, 3 (4 if homogeneous), height, width]\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, height, width = list(depth.shape)\n",
    "    depth = depth.reshape(batch, 1, -1)\n",
    "    pixel_coords = pixel_coords.reshape(batch, 3, -1)\n",
    "    cam_coords = torch.mul(torch.inverse(intrinsics), pixel_coords) * depth\n",
    "    \n",
    "    if is_homogeneous:\n",
    "        ones = torch.ones(batch,1, height*width)\n",
    "        cam_coords = torch.cat((cam_coords, ones),1)\n",
    "    cam_coords = torch.reshape(batch, -1, height, width)\n",
    "    \n",
    "    return cam_coords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SFM Learner-Needs changing\n",
    "# def pixel2cam(depth, pixel_coords,intrinsics_inv):\n",
    "#     global pixel_coords\n",
    "#     \"\"\"Transform coordinates in the pixel frame to the camera frame.\n",
    "#     Args:\n",
    "#         depth: depth maps -- [B, H, W]\n",
    "#         intrinsics_inv: intrinsics_inv matrix for each element of batch -- [B, 3, 3]\n",
    "#         is_homogeneous: return homogeneous coordinates\n",
    "#     Returns:\n",
    "#         array of (u,v,1) cam coordinates -- [B, 3, H, W]\n",
    "#     \"\"\"\n",
    "#     b, h, w = depth.size()\n",
    "#     if (pixel_coords is None) or pixel_coords.size(2) < h:\n",
    "#         set_id_grid(depth)\n",
    "#     current_pixel_coords = pixel_coords[:,:,:h,:w].expand(b,3,h,w).reshape(b, 3, -1)  # [B, 3, H*W]\n",
    "#     cam_coords = (intrinsics_inv @ current_pixel_coords).reshape(b, 3, h, w)\n",
    "#     return cam_coords * depth.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam2pixel(cam_coords, proj):\n",
    "    \"\"\"Transforms coordinates in a camera frame to the pixel frame.\n",
    "\n",
    "        Args:\n",
    "        cam_coords: [batch, 4, height, width]\n",
    "        proj: [batch, 4, 4]\n",
    "        Returns:\n",
    "        Pixel coordinates projected from the camera frame [batch, height, width, 2]\n",
    "    \"\"\"\n",
    "    batch, _, height, width = list(cam_coords.shape)\n",
    "    unnormalized_pixel_coords = torch.mul(proj, cam_coords)\n",
    "    x_u = unnormalized_pixel_coords[0:-1, 0:1, 0:-1]\n",
    "    y_u = unnormalized_pixel_coords[0:-1, 1:1, 0:-1]\n",
    "    z_u = unnormalized_pixel_coords[0:-1, 2:1, 0:-1]\n",
    "    x_n = x_u/(z_u + 1e-10)\n",
    "    y_n = y_u/(z_u + 1e-10)\n",
    "    pixel_coords = torch.cat((x_n, y_n), 1)\n",
    "    pixel_coords = pixel_coords.reshape(batch, 2, height, width)\n",
    "    z_u = z_u.reshape(batch, height, width, 1)\n",
    "    \n",
    "    return pixel_coords.permute(0,2,3,1), z_u\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SFM Learner-Needs changing\n",
    "# def cam2pixel(cam_coords, proj_c2p_rot, proj_c2p_tr):\n",
    "#     \"\"\"Transform coordinates in the camera frame to the pixel frame.\n",
    "#     Args:\n",
    "#         cam_coords: pixel coordinates defined in the first camera coordinates system -- [B, 4, H, W]\n",
    "#         proj_c2p_rot: rotation matrix of cameras -- [B, 3, 4]\n",
    "#         proj_c2p_tr: translation vectors of cameras -- [B, 3, 1]\n",
    "#     Returns:\n",
    "#         array of [-1,1] coordinates -- [B, 2, H, W]\n",
    "#     \"\"\"\n",
    "#     b, _, h, w = cam_coords.size()\n",
    "#     cam_coords_flat = cam_coords.reshape(b, 3, -1)  # [B, 3, H*W]\n",
    "#     if proj_c2p_rot is not None:\n",
    "#         pcoords = proj_c2p_rot @ cam_coords_flat\n",
    "#     else:\n",
    "#         pcoords = cam_coords_flat\n",
    "\n",
    "#     if proj_c2p_tr is not None:\n",
    "#         pcoords = pcoords + proj_c2p_tr  # [B, 3, H*W]\n",
    "#     X = pcoords[:, 0]\n",
    "#     Y = pcoords[:, 1]\n",
    "#     Z = pcoords[:, 2].clamp(min=1e-3)\n",
    "\n",
    "#     X_norm = 2*(X / Z)/(w-1) - 1  # Normalized, -1 if on extreme left, 1 if on extreme right (x = w-1) [B, H*W]\n",
    "#     Y_norm = 2*(Y / Z)/(h-1) - 1  # Idem [B, H*W]\n",
    "\n",
    "#     pixel_coords = torch.stack([X_norm, Y_norm], dim=2)  # [B, H*W, 2]\n",
    "#     return pixel_coords.reshape(b,2,h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid(batch, height, width):\n",
    "    \n",
    "    \"\"\"Construct a 2D meshgrid.\n",
    "  Args:\n",
    "    batch: batch size\n",
    "    height: height of the grid\n",
    "    width: width of the grid\n",
    "    is_homogeneous: whether to return in homogeneous coordinates\n",
    "  Returns:\n",
    "    x,y grid coordinates [batch, 2 (3 if homogeneous), height, width]\n",
    "  \"\"\"\n",
    "    x_t=torch.matmul(torch.ones([height,1]), torch.transpose(torch.unsqueeze(torch.linspace(-1,1,width),1),1,0))\n",
    "    y_t=torch.matmul(torch.unsqueeze(torch.linspace(-1,1,height),1), torch.ones([1, width]))\n",
    "    x_t = (x_t + 1.0) * 0.5 * torch.tensor(width - 1).float()\n",
    "    y_t = (y_t + 1.0) * 0.5 * torch.tensor(height - 1).float()\n",
    "    coords=torch.stack((x_t,y_t),0)\n",
    "    coords=torch.repeat_interleave(torch.unsqueeze(coords,0), batch, dim=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dims checked, wont need changing\n",
    "def inverse_warp(img, depth, pose, intrinsics, format='eular'):\n",
    "    \"\"\"Inverse warp a source image to the target image plane based on projection\n",
    "    Args:\n",
    "    img: the source image [batch, height_s, width_s, 3]\n",
    "    depth: depth map of the target image [batch, height_t, width_t]\n",
    "    pose: target to source camera transformation matrix [batch, 6], in the\n",
    "          order of tx, ty, tz, rx, ry, rz\n",
    "    intrinsics: camera intrinsics [batch, 3, 3]\n",
    "    Returns:\n",
    "    Source image inverse warped to the target image plane [batch, height_t,\n",
    "    width_t, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    batch, height, width,_ = img.size()\n",
    "\n",
    "    #Pose estimate:\n",
    "    pose = pose_vec2mat(pose,format)\n",
    "\n",
    "    #pixel_grid\n",
    "    pixel_coords = meshgrid(batch, height, width)\n",
    "\n",
    "    #pixel_coords->camera frame\n",
    "    cam_coords = pixel2cam(depth, pixel_coords, intrinsics)\n",
    "\n",
    "    #Intrinsic Matrix-4x4\n",
    "    #TODO: Can you make it 3x4\n",
    "    filler=torch.tensor([[[0,0,0,1]]])\n",
    "    filler=torch.repeat_interleave(filler, batch, dim=0)\n",
    "    intrinsics=torch.cat((intrinsics, torch.zeros([batch,3,1])), 2)\n",
    "    intrinsics=torch.cat((intrinsics, filler), 1)\n",
    "\n",
    "    #Project=Target-Cam->source-pixel-frame\n",
    "    proj_tgt_cam_2_src_pixel=torch.matmul(intrinsics, pose)\n",
    "    src_pixel_coords, src_depth=cam2pixel(cam_coords, proj_tgt_cam_2_src_pixel)\n",
    "\n",
    "    rigid_flow=src_pixel_coords-pixel_coords[:,0:2,:,:].permute(0,2,3,1)\n",
    "    output_img,wmask = bilinear_sampler(img, src_pixel_coords)\n",
    "    return output_img, wmask, rigid_flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not Needed\n",
    "def extract_image(img, src_pixel_coords,src_depth):\n",
    "    \n",
    "    batch, height, width, _=img.size()\n",
    "    out_img=torch.zeros([batch, height, width, 3])\n",
    "    out_depth=torch.zeros([batch,height, width, 1])\n",
    "    \n",
    "    return out_img, out_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #skipped it\n",
    "# def optflow_wrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def billinear_sampler(imgs, coords):\n",
    "    \"\"\"Construct a new image by bilinear sampling from the input image.\n",
    "\n",
    "    Points falling outside the source image boundary have value 0.\n",
    "\n",
    "    Args:\n",
    "    imgs: source image to be sampled from [batch, height_s, width_s, channels]\n",
    "    coords: coordinates of source pixels to sample from [batch, height_t,\n",
    "      width_t, 2]. height_t/width_t correspond to the dimensions of the output\n",
    "      image (don't need to be the same as height_s/width_s). The two channels\n",
    "      correspond to x and y coordinates respectively.\n",
    "    Returns:\n",
    "    A new sampled image [batch, height_t, width_t, channels]\n",
    "    \"\"\"\n",
    "    def _repeat(x, n_repeats):\n",
    "        rep=torch.transpose((torch.unsqueeze(torch.ones([n_repeats]),1)),1,0).float()\n",
    "        x=torch.matmul(torch.reshape(x, (-1,1)),rep)\n",
    "        \n",
    "        return x.reshape(x,[-1])\n",
    "    \n",
    "    #Was originally with tf.name_scope('image sampling')\n",
    "    def forward():\n",
    "        coords_x, coords_y = torch.split(coords, [1,1], dim=3).float()\n",
    "        inp_size=img.shape\n",
    "        coord_size=coords.shape\n",
    "        out_size=coords.shape\n",
    "        out_size[3]=imgs.shape[3]\n",
    "        \n",
    "        x0=torch.floor(coords_x)\n",
    "        x1=x0+1\n",
    "        y0=torch.floor(coords_y)\n",
    "        y1=y0+1\n",
    "        \n",
    "        y_max=(imgs.shape[1]-1).float()\n",
    "        x_max=(imgs.shape[2]-1).float()\n",
    "        zeros=torch.zeros([1]).float()\n",
    "        \n",
    "        x0_safe=torch.clamp(x0, zero, x_max)\n",
    "        y0_safe=torch.clamp(y0, zero, y_max)\n",
    "        x1_safe=torch.clamp(x1, zero, x_max)\n",
    "        y1_safe=torch.clamp(y1, zero, y_max)\n",
    "        \n",
    "        wt_x0=(x1-coords_x)*torch.equal(x0, x0_safe)\n",
    "        wt_x1=(coords_x-x0)*torch.equal(x1, x1_safe)\n",
    "        wt_y0=(y1-coords_y)*torch.equal(y0, y0_safe)\n",
    "        wt_y1=(coords_y-y0)*torch.equal(y1, y1_safe)\n",
    "        \n",
    "        dim1=tensor.double(inp_size[2]*inp_size[1])\n",
    "        dim2=tensor.double(inp_size[2])\n",
    "        \n",
    "        \n",
    "        base=torch.reshape(_repeat(coord_size[0]*dim1*coord_size[1]*coord_size[2]),[out_size[0], out_size[1], out_size[2], 1])\n",
    "        base_y0=base+y0_safe*dim2\n",
    "        base_y1=base+y1_safe*dim2\n",
    "        idx00=torch.reshape((x0_safe+base_y0),-1)\n",
    "        idx01=x0_safe+base_y1\n",
    "        idx10=x1_safe+base_y0\n",
    "        idx11=x1_safe+base_y1\n",
    "        \n",
    "        imgs_flat=torch.reshape(imgs, torch.stack((torch.tensor(-1),torch.tensor(inp_size[3])))).float()\n",
    "        \n",
    "        im00=torch.reshape(imgs_flat[idx00],out_size)\n",
    "        im01=torch.reshape(imgs_flat[idx01],out_size)\n",
    "        im10=torch.reshape(imgs_flat[idx10],out_size)\n",
    "        im11=torch.reshape(imgs_flat[idx11],out_size)\n",
    "        \n",
    "        w00=wt_x0*wt_y0\n",
    "        w01=wt_x0*wt_y1\n",
    "        w10=wt_x1*wt*y_0\n",
    "        w11=wt_x1*wt_y1\n",
    "        \n",
    "        output=torch.add(torch.tensor([w00*im00, w01*im01]),torch.tensor([w10 * im10, w11 * im11]))\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Skipped it\n",
    "# def depth_optlfow():\n",
    "#     #left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flow_to_image(flow):\n",
    "    \"\"\"\n",
    "    Convert flow into middlebury color code image\n",
    "    :param flow: optical flow map\n",
    "    :return: optical flow image in middlebury color\n",
    "    \"\"\"\n",
    "    u = flow[:, :, 0]\n",
    "    v = flow[:, :, 1]\n",
    "\n",
    "    maxu = -999.\n",
    "    maxv = -999.\n",
    "    minu = 999.\n",
    "    minv = 999.\n",
    "\n",
    "    u[u>minu] = minu\n",
    "    u[u<maxu] = maxu\n",
    "\n",
    "    v[v>minv] = minv\n",
    "    v[v<maxv] = maxv\n",
    "\n",
    "    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n",
    "    u[idxUnknow] = 0\n",
    "    v[idxUnknow] = 0\n",
    "\n",
    "    maxu = max(maxu, np.max(u))\n",
    "    minu = min(minu, np.min(u))\n",
    "\n",
    "    maxv = max(maxv, np.max(v))\n",
    "    minv = min(minv, np.min(v))\n",
    "\n",
    "    rad = np.sqrt(u ** 2 + v ** 2)\n",
    "    maxrad = max(-1, np.max(rad))\n",
    "\n",
    "    #print \"max flow: %.4f\\nflow range:\\nu = %.3f .. %.3f\\nv = %.3f .. %.3f\" % (maxrad, minu,maxu, minv, maxv)\n",
    "\n",
    "    u = u/(maxrad + np.finfo(float).eps)\n",
    "    v = v/(maxrad + np.finfo(float).eps)\n",
    "\n",
    "    img = compute_color(u, v)\n",
    "\n",
    "    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n",
    "    img[idx] = 0\n",
    "\n",
    "    return np.uint8(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_plasma(depth):\n",
    "    cmap = plt.get_cmap('plasma')\n",
    "\n",
    "    rgb_depth = cmap(depth/np.max(depth))\n",
    "    return np.float32(rgb_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_color(u, v):\n",
    "    \"\"\"\n",
    "    compute optical flow color map\n",
    "    :param u: optical flow horizontal map\n",
    "    :param v: optical flow vertical map\n",
    "    :return: optical flow in color code\n",
    "    \"\"\"\n",
    "    [h, w] = u.shape\n",
    "    img = np.zeros([h, w, 3])\n",
    "    nanIdx = np.isnan(u) | np.isnan(v)\n",
    "    u[nanIdx] = 0\n",
    "    v[nanIdx] = 0\n",
    "\n",
    "    colorwheel = make_color_wheel()\n",
    "    ncols = np.size(colorwheel, 0)\n",
    "\n",
    "    rad = np.sqrt(u**2+v**2)\n",
    "\n",
    "    a = np.arctan2(-v, -u) / np.pi\n",
    "\n",
    "    fk = (a+1) / 2 * (ncols - 1) + 1\n",
    "\n",
    "    k0 = np.floor(fk).astype(int)\n",
    "\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols+1] = 1\n",
    "    f = fk - k0\n",
    "\n",
    "    for i in range(0, np.size(colorwheel,1)):\n",
    "        tmp = colorwheel[:, i]\n",
    "        col0 = tmp[k0-1] / 255\n",
    "        col1 = tmp[k1-1] / 255\n",
    "        col = (1-f) * col0 + f * col1\n",
    "\n",
    "        idx = rad <= 1\n",
    "        col[idx] = 1-rad[idx]*(1-col[idx])\n",
    "        notidx = np.logical_not(idx)\n",
    "\n",
    "        col[notidx] *= 0.75\n",
    "        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_wheel():\n",
    "    \"\"\"\n",
    "    Generate color wheel according Middlebury color code\n",
    "    :return: Color wheel\n",
    "    \"\"\"\n",
    "    RY = 15\n",
    "    YG = 6\n",
    "    GC = 4\n",
    "    CB = 11\n",
    "    BM = 13\n",
    "    MR = 6\n",
    "\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "\n",
    "    colorwheel = np.zeros([ncols, 3])\n",
    "\n",
    "    col = 0\n",
    "\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n",
    "    col += RY\n",
    "\n",
    "    # YG\n",
    "    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n",
    "    colorwheel[col:col+YG, 1] = 255\n",
    "    col += YG\n",
    "\n",
    "    # GC\n",
    "    colorwheel[col:col+GC, 1] = 255\n",
    "    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n",
    "    col += GC\n",
    "\n",
    "    # CB\n",
    "    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n",
    "    colorwheel[col:col+CB, 2] = 255\n",
    "    col += CB\n",
    "\n",
    "    # BM\n",
    "    colorwheel[col:col+BM, 2] = 255\n",
    "    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n",
    "    col += + BM\n",
    "\n",
    "    # MR\n",
    "    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
    "    colorwheel[col:col+MR, 0] = 255\n",
    "\n",
    "    return colorwheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Skipped it\n",
    "# def detect_reflection(img):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
