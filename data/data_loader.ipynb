{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import cv2 as cv\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_normalize_image(img):\n",
    "\n",
    "    gray = cv.cvtColor(img, cv.COLOR_RGB2GRAY)\n",
    "    float_gray = gray.astype(np.float32) / 255.0\n",
    "\n",
    "    blur = cv.GaussianBlur(float_gray, (0, 0), sigmaX=2, sigmaY=2)\n",
    "    num = float_gray - blur\n",
    "\n",
    "    blur = cv.GaussianBlur(num*num, (0, 0), sigmaX=20, sigmaY=20)\n",
    "    den = cv.pow(blur, 0.5)+0.0000001\n",
    "\n",
    "    gray = num / den\n",
    "\n",
    "    cv.normalize(gray, dst=gray, alpha=0.0, beta=1.0, norm_type=cv.NORM_MINMAX)\n",
    "\n",
    "    gray = np.concatenate((gray[:,:,np.newaxis],gray[:,:,np.newaxis],gray[:,:,np.newaxis]),axis=2)\n",
    "\n",
    "    return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "    \"\"\" Data loading class for training heatmap-attention-padding network\n",
    "\n",
    "    Args:\n",
    "        dataset_dir: Folder contain .tfrecords files\n",
    "        batch_size: training batch size\n",
    "        image_height, image_width: input image height and width\n",
    "        opt: flags from input parser\n",
    "    \n",
    "    Returns:\n",
    "        new_mask: A gauss smoothed tensor\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_dir, batch_size, image_height, image_width, num_epochs, num_views):\n",
    "        self.dataset_dir=dataset_dir\n",
    "        self.batch_size=batch_size\n",
    "        self.image_height=image_height\n",
    "        self.image_width=image_width\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_views = num_views\n",
    "        \n",
    "    def inputs(self, is_training=True):\n",
    "        \"\"\"Reads input data num_epochs times.\n",
    "        Args:\n",
    "            batch_size: Number of examples per returned batch.\n",
    "            num_epochs: Number of times to read the input data, or 0/None to\n",
    "            train forever.\n",
    "        Returns:\n",
    "            data_dict: A dictional contain input image and groundtruth label\n",
    "        \"\"\"\n",
    "        data_dict={}\n",
    "        image_seq_list=[]\n",
    "        image_seq_norm_list=[]\n",
    "        depth_seq_list=[]\n",
    "        intrinsics_list=[]\n",
    "        \n",
    "        def loader(example_pkl):\n",
    "            image_seq=example_pkl['image_seq']\n",
    "            depth_seq=example_pkl['depth_seq']\n",
    "            intrinsics=example_pkl['intrinsics']\n",
    "            image_seq_norm=image_seq.copy()\n",
    "            \n",
    "            for i in range (0, len(img_seq)):\n",
    "                image_seq_norm[i]=local_normalize_image(image_seq_norm[i])\n",
    "                image_seq_norm[i]=torch.reshape(image_seq_norm[i],(image_height, image_width*num_views,3))\n",
    "                \n",
    "            data_dict={}\n",
    "            data_dict['image_seq']=image_seq\n",
    "            data_dict['image_seq_norm']=image_seq_norm\n",
    "            data_dict['depth_seq']=depth_seq\n",
    "            data_dict['intrinsics']=intrinsics\n",
    "            \n",
    "            if is_training:\n",
    "                data_dict=self.data_augmentation2(data_dict, self.image_height, self.image_width)\n",
    "            \n",
    "            return data_dict\n",
    "        \n",
    "        if not self.num_epochs:\n",
    "            self.num_epochs=None\n",
    "            \n",
    "        filenames=glob.glob(os.path.join(self.dataset_dir, '*.pickle'))\n",
    "        from random import shuffle\n",
    "        shuffle(filenames)\n",
    "        \n",
    "        datasets=Dataloader(dataset=filenames, batch_size=3, shuffle=True)\n",
    "        #Complete This\n",
    "    \n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation2(self, data_dict, out_h, out_w, is_training=True):\n",
    "    \n",
    "    def flip_intrinsics(intrinsics, width):\n",
    "        \n",
    "        fx=intrinsics[0,0]\n",
    "        fy=intrinsics[1,1]\n",
    "        cx=width-intrinsics[0,2]\n",
    "        cy=intrinsics[1,2]\n",
    "        \n",
    "        zeros=torch.zeros(fx.shape)\n",
    "        r1=torch.stack([fx,zeros,cx])\n",
    "        r2=torch.stack([zeros, fy, cy])\n",
    "        r3=torch.tensor([0,0,1])\n",
    "        intrinsics=torch.stack([r1,r2,r3], axis=0)\n",
    "        \n",
    "        return intrinscs\n",
    "    \n",
    "    def flip_left_right(image_seq, num_views):\n",
    "        \"\"\"Perform random distortions on an image.\n",
    "            Args:\n",
    "            image: A float32 Tensor of shape [height, width, 3] with values in [0, 1).\n",
    "            thread_id: Preprocessing thread id used to select the ordering of color\n",
    "              distortions. There should be a multiple of 2 preprocessing threads.\n",
    "            Returns:\n",
    "            distorted_image: A float32 Tensor of shape [height, width, 3] with values in\n",
    "              [0, 1].\n",
    "            \"\"\"\n",
    "        in_h, in_w, _ = image_seq.shape\n",
    "        in_w=in_w/num_views\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            \n",
    "            image=image_seq[0:-1, int(in_w)*i:int(in_w), 0:-1]\n",
    "            image=flip_left_right(image)\n",
    "            \n",
    "            if i==0:\n",
    "                flip_image=image\n",
    "            else:\n",
    "                flip_image=torch.cat([flip_image, image], axix=1)\n",
    "                \n",
    "        return flip_image\n",
    "    \n",
    "    def random_scaling(data_dict, num_views):\n",
    "        in_h, in_w=data_dict['image_seq'][0].shape\n",
    "        in_w = in_w/num_views\n",
    "        \n",
    "        scaling=torch.rand((1))\n",
    "        x_scaling=scaling[0]\n",
    "        y_scaling=scaling[0]\n",
    "        out_h=torch.tensor(in_h*y_scaling).type('torch.IntTensor')\n",
    "        out_w=torch.tensor(in_w*x_scaling).type('torch.IntTensor')\n",
    "        \n",
    "        scaled_image=[]\n",
    "        scaled_depths=[]\n",
    "        scaled_images_norm=[]\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            \n",
    "            image=data_dict['image_seq'][i][0:-1, int(in_w)*i: int(in_w), 0:-1]\n",
    "            image=image.transforms.CenterCrop((out_h,out_w))\n",
    "            scaled_images.append(image)\n",
    "            \n",
    "            image_norm=data_dict['image_seq_norm'][i][0:-1, int(in_w)*i:int(in_w), 0:-1]   \n",
    "            image_norm=F.interpolate(image_norm,(out_h, out_w))\n",
    "            scaled_images_norm.append(image_norm)\n",
    "            \n",
    "            depth=data_dict['depth_seq'][i][0:-1, int(in_w)*i:int(in_w), 0:-1]\n",
    "            depth=F.interpolate(depth,(out_h, out_w))\n",
    "            scaled_depths.append(depth)\n",
    "            \n",
    "        return scaled_images, scaled_depths, scaled_images_norm\n",
    "    \n",
    "    \n",
    "    def random_cropping(data_dict, scaled_images, scaled_depths, scaled_images_norm, num_views, out_h, out_w):\n",
    "        in_h, in_w=torch.unbind(scaled_image[0].shape)\n",
    "        \n",
    "        offset_y=torch.rand([1], 0, in_h - out_h+1)\n",
    "        offset_x=torch.rand([1], 0, in_w - out_w+1)\n",
    "        \n",
    "        _in_h=in_h.float32\n",
    "        _in_w=in_w.float32\n",
    "        _out_h=out_w.float32\n",
    "        _out_w=out_w.float32\n",
    "        \n",
    "        fx = data_dict['intrinsics'][0,0]*_in_w/_out_w\n",
    "        fy = data_dict['intrinsics'][1,1]*_in_h/_out_h\n",
    "        cx = data_dict['intrinsics'][0,2]*_in_w/_out_w-offest_x.float32\n",
    "        cy = data_dict['intrinsics'][0,2]*_in_h/_out_h-offest_y.float32\n",
    "        \n",
    "        zeros=torch.zeros(fx.shape)\n",
    "        r1=torch.stack([fx,zeros,cx])\n",
    "        r2=torch.stack([zeros, fy, cy])\n",
    "        r3=torch.tensor([0,0,1])\n",
    "        data_dict['intrinsics']=torch.stack([r1,r2,r3], axis=0)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            \n",
    "            if i==0:\n",
    "                cropped_images = scaled_image[i][offset_y:offset_x, out_h:out_w]\n",
    "                cropped_depths = scaled_depths[i][offset_y:offset_x, out_h:out_w]\n",
    "                cropped_images_norm=scaled_images_norm[i][offset_y:offset_x, out_h:out_w]\n",
    "            else:\n",
    "                cropped_images=torch.cat(cropped_images, scaled_image[i][offset_y:offset_x, out_h:out_w], dim=1)\n",
    "                cropped_depths = torch.cat(cropped_depths, scaled_depths[i][offset_y:offset_x, out_h:out_w], dim=1)\n",
    "                cropped_images_norm=torch.cat(cropped_images_norm, scaled_images_norm[i][offset_y:offset_x, out_h:out_w], dim=1)\n",
    "                \n",
    "        data_dict['image_seq'] = cropped_images\n",
    "        data_dict['depth_Seq'] = cropped_depths\n",
    "        data_dict['image_seq_norm'] = cropped_images_norm\n",
    "        \n",
    "        return data_dict\n",
    "    \n",
    "    scaled_images, scaled_depths, scaled_images_norm = random_scaling(data_dict, self.num_views)\n",
    "    \n",
    "    data_dict = random_cropping(data_dict, scaled_images, scaled_depths, scaled_images_norm, self.num_views, out_h, out_w)\n",
    "    \n",
    "    return data_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
