{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import BasicConvLSTMCell\n",
    "\n",
    "import torchvision\n",
    "DISP_SCALING_RESNET50 = 10.0\n",
    "MIN_DISP = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_like(inputs,ref):\n",
    "    assert(input.size(2) >= ref.size(2) and input.size(3) >= ref.size(3))\n",
    "    \n",
    "    return input[:, :, :ref.size(2), :ref.size(3)]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convLSTM(input, hidden, filters, kernel, scope):\n",
    "    cell = BasicConvLSTMCell.BasicConvLSTMCell([input.get_shape()[1], input.get_shape()[2]], kernel, filters)\n",
    "    \n",
    "    if hidden is None:\n",
    "        hidden=cell.zero_state(input.shape[0]).float()\n",
    "    \n",
    "    y_, hideen=cell(input, hidden)\n",
    "    \n",
    "    return y_, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_depth_net_decoderlstm(self, current_input, hidden_state, is_training=True):\n",
    "    H=current_input.shape[1].value\n",
    "    W=current_input.shape[2].value\n",
    "    \n",
    "    def forward():\n",
    "        cnv1=nn.Conv2d(current_input, 32, [3,3], stride=2, scope='cnv1')\n",
    "        cnv1b=nn.Conv2d(cnv1 ,32, [3,3], rate=2, stride=1, scope='cnv1b')\n",
    "        cnv2=nn.Conv2d(cnv1b, 64, [3,3], stride=2, scope='cnv2')\n",
    "        cnv2b = nn.Conv2d(cnv2,  64,  [3, 3], rate=2, stride=1, scope='cnv2b')\n",
    "        cnv3  = nn.Conv2d(cnv2b, 128, [3, 3], stride=2, scope='cnv3')\n",
    "        cnv3b = nn.Conv2d(cnv3,  128, [3, 3], rate=2, stride=1, scope='cnv3b')\n",
    "        cnv4  = nn.Conv2d(cnv3b, 256, [3, 3], stride=2, scope='cnv4')\n",
    "        cnv4b = nn.Conv2d(cnv4,  256, [3, 3], rate=2, stride=1, scope='cnv4b')\n",
    "        cnv5  = nn.Conv2d(cnv4b, 256, [3, 3], stride=2, scope='cnv5')\n",
    "        cnv5b = nn.Conv2d(cnv5,  256, [3, 3], rate=2, stride=1, scope='cnv5b')\n",
    "        cnv6  = nn.Conv2d(cnv5b, 256, [3, 3], stride=2, scope='cnv6')\n",
    "        cnv6b = nn.Conv2d(cnv6,  256, [3, 3], rate=2, stride=1, scope='cnv6b')\n",
    "        cnv7  = nn.Conv2d(cnv6b, 512, [3, 3], stride=2, scope='cnv7')\n",
    "        cnv7b = nn.Conv2d(cnv7,  512, [3, 3], rate=2, stride=1, scope='cnv7b')\n",
    "        \n",
    "        upcnv7 = nn.ConvTranspose2d(cnv7b, 256, [3, 3], stride=2, scope='upcnv7')\n",
    "        # There might be dimension mismatch due to uneven down/up-sampling\n",
    "        upcnv7 = resize_like(upcnv7, cnv6b)\n",
    "        i7_in  = torch.concat((upcnv7, cnv6b), axis=3)\n",
    "        icnv7, hidden7= convLSTM(i7_in, hidden_state[6], 256, [3, 3], scope='icnv7_lstm')\n",
    "        upcnv6 = nn.ConvTranspose2d(icnv7, 128, [3, 3], stride=2, scope='upcnv6')\n",
    "        upcnv6 = resize_like(upcnv6, cnv5b)\n",
    "        i6_in  = torc.cat((upcnv6, cnv5b), axis=3)\n",
    "        icnv6, hidden6= convLSTM(i6_in, hidden_state[5], 128, [3, 3], scope='icnv6_lstm')\n",
    "\n",
    "        upcnv5 = nn.ConvTranspose2d(icnv6, 128, [3, 3], stride=2, scope='upcnv5')\n",
    "        upcnv5 = resize_like(upcnv5, cnv4b)\n",
    "        i5_in  = torch.cat((upcnv5, cnv4b), axis=3)\n",
    "        icnv5, hidden5 = convLSTM(i5_in, hidden_state[4], 128, [3, 3], scope='icnv5_lstm')\n",
    "        upcnv4 = nn.ConvTranspose2d(icnv5, 128, [3, 3], stride=2, scope='upcnv4')\n",
    "        i4_in  = torch.cat((upcnv4, cnv3b), axis=3)\n",
    "        icnv4, hidden4 = convLSTM(i4_in, hidden_state[3], 128, [3, 3], scope='icnv4_lstm')\n",
    "        upcnv3 = nn.ConvTranspose2d(icnv4, 64,  [3, 3], stride=2, scope='upcnv3')\n",
    "        i3_in  = torch.cat((upcnv3, cnv2b), axis=3)\n",
    "        icnv3, hidden3 = convLSTM(i3_in, hidden_state[2], 64, [3, 3], scope='icnv3_lstm')\n",
    "        upcnv2 = nn.ConvTranspose2d(icnv3, 32,  [3, 3], stride=2, scope='upcnv2')\n",
    "        i2_in  = torch.cat((upcnv2, cnv1b), axis=3)\n",
    "        icnv2, hidden2 = convLSTM(i2_in, hidden_state[1], 32, [3, 3], scope='icnv2_lstm')\n",
    "        \n",
    "        upcnv1 = nn.ConvTranspose2d(icnv2, 16,  [3, 3], stride=2, scope='upcnv1')\n",
    "        icnv1, hidden1 = convLSTM(upcnv1, hidden_state[0], 16, [3, 3], scope='icnv1_lstm')\n",
    "        depth=nn.Conv2d(icnv1,1,[3,3], stride=1, activation=nn.sigmoid, scope='disp1')*DISP_SCALING_RESNET50+MIN_DISP\n",
    "                       \n",
    "        return depth, [hidden1, hidden2, hidden3, hidden4, hidden5, hidden6, hidden7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_net(posenet_inputs, hidden_state, is_training=True):\n",
    "    \n",
    "    def forward():\n",
    "        conv1  = nn.Conv2d(posenet_inputs, 16,  7, 2)\n",
    "        cnv1b, hidden1 = convLSTM(conv1, hidden_state[0], 16, [3, 3], scope='cnv1_lstm')\n",
    "        conv2  = nn.Conv2d(cnv1b, 32,  5, 2)\n",
    "        cnv2b, hidden2 = convLSTM(conv2, hidden_state[1], 64, [3, 3], scope='cnv2_lstm')\n",
    "        conv3  = nn.Conv2d(cnv2b, 64,  3, 2)\n",
    "        cnv3b, hidden3 = convLSTM(conv3, hidden_state[2], 128, [3, 3], scope='cnv3_lstm')\n",
    "        conv4  = nn.Conv2d(cnv3b, 128, 3, 2)\n",
    "        cnv4b, hidden4 = convLSTM(conv4, hidden_state[3], 256, [3, 3], scope='cnv4_lstm')\n",
    "        conv5  = nn.Conv2d(cnv4b, 256, 3, 2)\n",
    "        cnv5b, hidden5 = convLSTM(conv5, hidden_state[4], 256, [3, 3], scope='cnv5_lstm')\n",
    "        conv6  = nn.Conv2d(cnv5b, 256, 3, 2)\n",
    "        cnv6b, hidden6 = convLSTM(conv6, hidden_state[5], 256, [3, 3], scope='cnv6_lstm')\n",
    "        conv7  = nn.Conv2d(cnv6b, 256, 3, 2)\n",
    "        cnv7b, hidden7 = convLSTM(conv7, hidden_state[6], 512, [3, 3], scope='cnv7_lstm')\n",
    "        pose_pred = nn.Conv2d(cnv7b, 6, 1, 1,activation=None)\n",
    "        pose_avg = torch.mean(pose_pred, [1, 2])\n",
    "        pose_final = torch.reshape(pose_avg, [-1, 6])*0.01\n",
    "\n",
    "        return pose_final,[hidden1, hidden2, hidden3, hidden4, hidden5, hidden6, hidden7]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_depth_net_encoderlstm(current_input,hidden_state,is_training=True):\n",
    "    \n",
    "    H = current_input.get_shape()[1].value\n",
    "    W = current_input.get_shape()[2].value\n",
    "\n",
    "    def forward():\n",
    "        cnv1  = nn.Conv2d(current_input, 32,  [3, 3], stride=2, scope='cnv1')\n",
    "        cnv1b, hidden1 = convLSTM(cnv1, hidden_state[0], 32, [3, 3], scope='cnv1_lstm')\n",
    "        #cnv1b = slim.conv2d(cnv1,  32,  [3, 3], rate=2, stride=1, scope='cnv1b')\n",
    "        cnv2  = nn.Conv2d(cnv1b, 64,  [3, 3], stride=2, scope='cnv2')\n",
    "        cnv2b, hidden2 = convLSTM(cnv2, hidden_state[1], 64, [3, 3], scope='cnv2_lstm')\n",
    "        #cnv2b = slim.conv2d(cnv2,  64,  [3, 3], rate=2, stride=1, scope='cnv2b')\n",
    "        cnv3  = nn.Conv2d(cnv2b, 128, [3, 3], stride=2, scope='cnv3')\n",
    "        cnv3b, hidden3 = convLSTM(cnv3, hidden_state[2], 128, [3, 3], scope='cnv3_lstm')\n",
    "        #cnv3b = slim.conv2d(cnv3,  128, [3, 3], rate=2, stride=1, scope='cnv3b')\n",
    "        cnv4  = nn.Conv2d(cnv3b, 256, [3, 3], stride=2, scope='cnv4')\n",
    "        cnv4b, hidden4 = convLSTM(cnv4, hidden_state[3], 256, [3, 3], scope='cnv4_lstm')\n",
    "        #cnv4b = slim.conv2d(cnv4,  256, [3, 3], rate=2, stride=1, scope='cnv4b')\n",
    "        cnv5  = nn.Conv2d(cnv4b, 256, [3, 3], stride=2, scope='cnv5')\n",
    "        cnv5b, hidden5 = convLSTM(cnv5, hidden_state[4], 256, [3, 3], scope='cnv5_lstm')\n",
    "        #cnv5b = slim.conv2d(cnv5,  256, [3, 3], rate=2, stride=1, scope='cnv5b')\n",
    "        cnv6  = nn.Conv2d(cnv5b, 256, [3, 3], stride=2, scope='cnv6')\n",
    "        cnv6b, hidden6 = convLSTM(cnv6, hidden_state[5], 256, [3, 3], scope='cnv6_lstm')\n",
    "        #cnv6b = slim.conv2d(cnv6,  256, [3, 3], rate=2, stride=1, scope='cnv6b')\n",
    "        cnv7  = nn.Conv2d(cnv6b, 512, [3, 3], stride=2, scope='cnv7')\n",
    "        cnv7b, hidden7 = convLSTM(cnv7, hidden_state[6], 512, [3, 3], scope='cnv7_lstm')\n",
    "        #cnv7b = slim.conv2d(cnv7,  512, [3, 3], rate=2, stride=1, scope='cnv7b')\n",
    "\n",
    "        upcnv7 = nn.ConvTranspose2d(cnv7b, 256, [3, 3], stride=2, scope='upcnv7')\n",
    "        # There might be dimension mismatch due to uneven down/up-sampling\n",
    "        upcnv7 = resize_like(upcnv7, cnv6b)\n",
    "        i7_in  = torch.cat((upcnv7, cnv6b), axis=3)\n",
    "        icnv7  = nn.Conv2d(i7_in, 256, [3, 3], stride=1, scope='icnv7')\n",
    "\n",
    "        upcnv6 = nn.ConvTranspose2d(icnv7, 128, [3, 3], stride=2, scope='upcnv6')\n",
    "        upcnv6 = resize_like(upcnv6, cnv5b)\n",
    "        i6_in  = torch.cat((upcnv6, cnv5b), axis=3)\n",
    "        icnv6  = nn.Conv2d(i6_in, 128, [3, 3], stride=1, scope='icnv6')\n",
    "\n",
    "        upcnv5 = nn.ConvTranspose2d(icnv6, 128, [3, 3], stride=2, scope='upcnv5')\n",
    "        upcnv5 = resize_like(upcnv5, cnv4b)\n",
    "        i5_in  = torch.cat([upcnv5, cnv4b], axis=3)\n",
    "        icnv5  = nn.Conv2d(i5_in, 128, [3, 3], stride=1, scope='icnv5')\n",
    "\n",
    "        upcnv4 = nn.ConvTranspose2d(icnv5, 128, [3, 3], stride=2, scope='upcnv4')\n",
    "        upcnv4 = resize_like(upcnv4, cnv3b)\n",
    "        i4_in  = torch.cat((upcnv4, cnv3b), axis=3)\n",
    "        icnv4  = nn.Conv2d(i4_in, 128, [3, 3], stride=1, scope='icnv4')\n",
    "\n",
    "        upcnv3 = nn.ConvTranspose2d(icnv4, 64,  [3, 3], stride=2, scope='upcnv3')\n",
    "        upcnv3 = resize_like(upcnv3, cnv2b)\n",
    "        i3_in  = torch.cat((upcnv3, cnv2b), axis=3)\n",
    "        icnv3  = nn.Conv2d(i3_in, 64, [3, 3], stride=1, scope='icnv3')\n",
    "\n",
    "        upcnv2 = nn.ConvTranspose2d(icnv3, 32,  [3, 3], stride=2, scope='upcnv2')\n",
    "        upcnv2 = resize_like(upcnv2, cnv1b)\n",
    "        i2_in  = torch.cat([upcnv2, cnv1b], axis=3)\n",
    "        icnv2  = nn.Conv2d(i2_in, 32, [3, 3], stride=1, scope='icnv2')\n",
    "\n",
    "        upcnv1 = nn.ConvTranspose2d(icnv2, 16,  [3, 3], stride=2, scope='upcnv1')\n",
    "        icnv1  = nn.Conv2d(upcnv1, 16,  [3, 3], stride=1, scope='icnv1')\n",
    "        depth  = nn.Conv2d(icnv1, 1,   [1, 1], stride=1,activation=nn.sigmoid, scope='disp1')*DISP_SCALING_RESNET50+MIN_DISP # was 10.0\n",
    "\n",
    "        return depth, [hidden1, hidden2, hidden3, hidden4, hidden5, hidden6, hidden7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
